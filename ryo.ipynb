{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from pathlib import Path\n",
    "import create_dataset as cd\n",
    "import feature_engineering as fe\n",
    "import os \n",
    "from prep import * \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tim = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_tim_copy.csv')\n",
    "data_sien = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_sien_copy.csv')\n",
    "data_tim2 = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_tim_2_copy.csv')\n",
    "data_sien2 = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_sien_copy.csv')\n",
    "\n",
    "data_tim = data_tim.drop(columns = ['Unnamed: 0'])\n",
    "data_sien = data_sien.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>X (m/s^2)</th>\n",
       "      <th>Y (m/s^2)</th>\n",
       "      <th>Z (m/s^2)</th>\n",
       "      <th>X (rad/s)</th>\n",
       "      <th>Y (rad/s)</th>\n",
       "      <th>Z (rad/s)</th>\n",
       "      <th>name_climber</th>\n",
       "      <th>grading</th>\n",
       "      <th>...</th>\n",
       "      <th>roll</th>\n",
       "      <th>pitch</th>\n",
       "      <th>Velocity_X</th>\n",
       "      <th>Velocity_Y</th>\n",
       "      <th>Velocity_Z</th>\n",
       "      <th>Displacement_X</th>\n",
       "      <th>Displacement_Y</th>\n",
       "      <th>Displacement_Z</th>\n",
       "      <th>distance</th>\n",
       "      <th>climb_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-07 13:14:40.068719685-02:00</td>\n",
       "      <td>108.088635</td>\n",
       "      <td>0.157283</td>\n",
       "      <td>-0.442686</td>\n",
       "      <td>-1.072861</td>\n",
       "      <td>0.039662</td>\n",
       "      <td>-0.432147</td>\n",
       "      <td>-0.123366</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.750253</td>\n",
       "      <td>0.134698</td>\n",
       "      <td>109.595352</td>\n",
       "      <td>76.050713</td>\n",
       "      <td>-76.762943</td>\n",
       "      <td>668.612258</td>\n",
       "      <td>-2206.086041</td>\n",
       "      <td>-2018.401478</td>\n",
       "      <td>3063.952105</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-07 13:14:40.270581102-02:00</td>\n",
       "      <td>108.289898</td>\n",
       "      <td>-0.659116</td>\n",
       "      <td>0.043920</td>\n",
       "      <td>-0.302016</td>\n",
       "      <td>-0.675243</td>\n",
       "      <td>-0.943927</td>\n",
       "      <td>-0.006866</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.997183</td>\n",
       "      <td>-1.137157</td>\n",
       "      <td>109.462097</td>\n",
       "      <td>76.059137</td>\n",
       "      <td>-76.823308</td>\n",
       "      <td>690.639274</td>\n",
       "      <td>-2190.766094</td>\n",
       "      <td>-2033.852136</td>\n",
       "      <td>3068.060168</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-07 13:14:40.472442519-02:00</td>\n",
       "      <td>108.491162</td>\n",
       "      <td>1.083198</td>\n",
       "      <td>-0.937163</td>\n",
       "      <td>0.811129</td>\n",
       "      <td>-0.707245</td>\n",
       "      <td>-0.285354</td>\n",
       "      <td>-0.007537</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.857363</td>\n",
       "      <td>0.718231</td>\n",
       "      <td>109.680650</td>\n",
       "      <td>75.870898</td>\n",
       "      <td>-76.660439</td>\n",
       "      <td>712.717472</td>\n",
       "      <td>-2175.506899</td>\n",
       "      <td>-2049.291228</td>\n",
       "      <td>3072.521928</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-07 13:14:40.674303937-02:00</td>\n",
       "      <td>108.692425</td>\n",
       "      <td>0.330704</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.293211</td>\n",
       "      <td>-0.433973</td>\n",
       "      <td>-0.436695</td>\n",
       "      <td>-0.142976</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092163</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>109.746664</td>\n",
       "      <td>75.875975</td>\n",
       "      <td>-76.601045</td>\n",
       "      <td>734.801873</td>\n",
       "      <td>-2160.225063</td>\n",
       "      <td>-2064.698002</td>\n",
       "      <td>3077.252664</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-07 13:14:40.876165354-02:00</td>\n",
       "      <td>108.893689</td>\n",
       "      <td>-0.816926</td>\n",
       "      <td>0.448517</td>\n",
       "      <td>-0.063428</td>\n",
       "      <td>-0.500497</td>\n",
       "      <td>-0.702771</td>\n",
       "      <td>0.051526</td>\n",
       "      <td>0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.711283</td>\n",
       "      <td>-1.064510</td>\n",
       "      <td>109.582791</td>\n",
       "      <td>75.966622</td>\n",
       "      <td>-76.614192</td>\n",
       "      <td>756.860595</td>\n",
       "      <td>-2144.946450</td>\n",
       "      <td>-2080.127939</td>\n",
       "      <td>3082.298733</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              datetime    Time (s)  X (m/s^2)  Y (m/s^2)  \\\n",
       "0  2023-06-07 13:14:40.068719685-02:00  108.088635   0.157283  -0.442686   \n",
       "1  2023-06-07 13:14:40.270581102-02:00  108.289898  -0.659116   0.043920   \n",
       "2  2023-06-07 13:14:40.472442519-02:00  108.491162   1.083198  -0.937163   \n",
       "3  2023-06-07 13:14:40.674303937-02:00  108.692425   0.330704   0.027100   \n",
       "4  2023-06-07 13:14:40.876165354-02:00  108.893689  -0.816926   0.448517   \n",
       "\n",
       "   Z (m/s^2)  X (rad/s)  Y (rad/s)  Z (rad/s)  name_climber  grading  ...  \\\n",
       "0  -1.072861   0.039662  -0.432147  -0.123366             0      4.3  ...   \n",
       "1  -0.302016  -0.675243  -0.943927  -0.006866             0      4.3  ...   \n",
       "2   0.811129  -0.707245  -0.285354  -0.007537             0      4.3  ...   \n",
       "3   0.293211  -0.433973  -0.436695  -0.142976             0      4.3  ...   \n",
       "4  -0.063428  -0.500497  -0.702771   0.051526             0      4.3  ...   \n",
       "\n",
       "       roll     pitch  Velocity_X  Velocity_Y  Velocity_Z  Displacement_X  \\\n",
       "0 -2.750253  0.134698  109.595352   76.050713  -76.762943      668.612258   \n",
       "1  2.997183 -1.137157  109.462097   76.059137  -76.823308      690.639274   \n",
       "2 -0.857363  0.718231  109.680650   75.870898  -76.660439      712.717472   \n",
       "3  0.092163  0.843308  109.746664   75.875975  -76.601045      734.801873   \n",
       "4  1.711283 -1.064510  109.582791   75.966622  -76.614192      756.860595   \n",
       "\n",
       "   Displacement_Y  Displacement_Z     distance  climb_id  \n",
       "0    -2206.086041    -2018.401478  3063.952105       1.1  \n",
       "1    -2190.766094    -2033.852136  3068.060168       1.1  \n",
       "2    -2175.506899    -2049.291228  3072.521928       1.1  \n",
       "3    -2160.225063    -2064.698002  3077.252664       1.1  \n",
       "4    -2144.946450    -2080.127939  3082.298733       1.1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_data = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/processed_data.csv')\n",
    "proc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('heartrate/heart_rate-2023-06-07 copy.json', 'r') as f1: \n",
    "    hr_json1 = json.load(f1)\n",
    "with open('heartrate/heart_rate-2023-06-14 copy.json', 'r') as f2: \n",
    "    hr_json2 = json.load(f2)\n",
    "\n",
    "\n",
    "# convert entries to datetime obj & put values in list \n",
    "hr_data1 = [] \n",
    "for heart_entry in hr_json1:\n",
    "    dt = heart_entry['dateTime']\n",
    "    hr = heart_entry['value']['bpm']\n",
    "    hr_data1.append((dt, hr))\n",
    "\n",
    "hr_data2 = []\n",
    "for heart_entry in hr_json2:\n",
    "    dt = heart_entry['dateTime']\n",
    "    hr = heart_entry['value']['bpm']\n",
    "    hr_data2.append((dt, hr))\n",
    "\n",
    "\n",
    "fitbit_datetimes = [x[0] for x in hr_data1]\n",
    "# convert fitbit times to datetime obj\n",
    "fitbit_datetimes = pd.to_datetime(fitbit_datetimes)\n",
    "fitbit_heart_rates = [x[1] for x in hr_data1]\n",
    "\n",
    "# prep for comparing datetimes \n",
    "proc_data['datetime'] = pd.to_datetime(proc_data['datetime'])\n",
    "proc_data['datetime'] =proc_data['datetime'].dt.tz_localize(None)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping_dict = {}\n",
    "\n",
    "# iterates through each snippet \n",
    "for group_name, group_data in proc_data.groupby(\"climb_id\"):\n",
    "    # gets all fitbit datetimes within snippet time interval \n",
    "    start_time = group_data.iloc[0]['datetime']\n",
    "    end_time = group_data.iloc[-1]['datetime']\n",
    "    fitbit_snippet_datetimes = fitbit_datetimes[(fitbit_datetimes >= start_time) & (fitbit_datetimes <= end_time)]\n",
    "    # match each fitbit datetime to nearest datetime in the processed dataframe \n",
    "    for fitbit_dt in fitbit_snippet_datetimes:\n",
    "        # datetime matching - CORRECT\n",
    "        \n",
    "        # proc_data['time_difference'] = abs(proc_data['datetime'] - fitbit_dt)\n",
    "        # closest_index = proc_data['time_difference'].idxmin()\n",
    "\n",
    "        # abs(proc_data['datetime'] - fitbit_dt)\n",
    "        closest_index = abs(proc_data['datetime'] - fitbit_dt).idxmin()\n",
    "\n",
    "        data_closest_dt = proc_data.loc[closest_index, 'datetime']\n",
    "        # get corresponding heart rate values & save mapping \n",
    "        hr_idx = fitbit_datetimes.get_loc(fitbit_dt)\n",
    "        mapping_dict[data_closest_dt] = fitbit_heart_rates[hr_idx] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new column by using HR to datetime mapping \n",
    "proc_data['heart-rate'] = proc_data['datetime'].map(mapping_dict)\n",
    "proc_data\n",
    "proc_data['heart-rate'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_num</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>X (m/s^2)</th>\n",
       "      <th>Y (m/s^2)</th>\n",
       "      <th>Z (m/s^2)</th>\n",
       "      <th>X (rad/s)</th>\n",
       "      <th>Y (rad/s)</th>\n",
       "      <th>Z (rad/s)</th>\n",
       "      <th>name_climber</th>\n",
       "      <th>grading</th>\n",
       "      <th>num_attempt</th>\n",
       "      <th>fall_top</th>\n",
       "      <th>outdoor</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>2023-06-07 16:36:31.299000-02:00</td>\n",
       "      <td>0.202257</td>\n",
       "      <td>0.616807</td>\n",
       "      <td>0.545096</td>\n",
       "      <td>-0.119170</td>\n",
       "      <td>-0.738735</td>\n",
       "      <td>-0.693665</td>\n",
       "      <td>-0.053444</td>\n",
       "      <td>0</td>\n",
       "      <td>4c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>2023-06-07 16:36:31.399774729-02:00</td>\n",
       "      <td>0.403552</td>\n",
       "      <td>0.427085</td>\n",
       "      <td>-0.449999</td>\n",
       "      <td>0.075464</td>\n",
       "      <td>-0.314339</td>\n",
       "      <td>-0.378076</td>\n",
       "      <td>0.015612</td>\n",
       "      <td>0</td>\n",
       "      <td>4c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>2023-06-07 16:36:31.500549459-02:00</td>\n",
       "      <td>0.604846</td>\n",
       "      <td>0.570240</td>\n",
       "      <td>0.152899</td>\n",
       "      <td>-0.213106</td>\n",
       "      <td>-0.273812</td>\n",
       "      <td>-0.442562</td>\n",
       "      <td>-0.130227</td>\n",
       "      <td>0</td>\n",
       "      <td>4c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>2023-06-07 16:36:31.601324189-02:00</td>\n",
       "      <td>0.806141</td>\n",
       "      <td>0.295633</td>\n",
       "      <td>-0.453686</td>\n",
       "      <td>0.244799</td>\n",
       "      <td>-0.260747</td>\n",
       "      <td>-0.114135</td>\n",
       "      <td>-0.019194</td>\n",
       "      <td>0</td>\n",
       "      <td>4c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>2023-06-07 16:36:31.702098919-02:00</td>\n",
       "      <td>1.007435</td>\n",
       "      <td>0.206272</td>\n",
       "      <td>-0.927811</td>\n",
       "      <td>-0.512616</td>\n",
       "      <td>-0.160211</td>\n",
       "      <td>-0.226173</td>\n",
       "      <td>-0.211577</td>\n",
       "      <td>0</td>\n",
       "      <td>4c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5186</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-06-14 17:02:20.736479365-02:00</td>\n",
       "      <td>62.783481</td>\n",
       "      <td>0.138590</td>\n",
       "      <td>-1.738872</td>\n",
       "      <td>-0.396787</td>\n",
       "      <td>-0.371624</td>\n",
       "      <td>-0.194527</td>\n",
       "      <td>-0.130351</td>\n",
       "      <td>1</td>\n",
       "      <td>5b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5187</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-06-14 17:02:20.938609523-02:00</td>\n",
       "      <td>62.984758</td>\n",
       "      <td>-0.756452</td>\n",
       "      <td>-0.380505</td>\n",
       "      <td>0.312611</td>\n",
       "      <td>0.064059</td>\n",
       "      <td>0.111573</td>\n",
       "      <td>-0.053014</td>\n",
       "      <td>1</td>\n",
       "      <td>5b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5188</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-06-14 17:02:21.140739682-02:00</td>\n",
       "      <td>63.186036</td>\n",
       "      <td>-0.321295</td>\n",
       "      <td>-0.189666</td>\n",
       "      <td>-0.070279</td>\n",
       "      <td>-0.062600</td>\n",
       "      <td>-0.059749</td>\n",
       "      <td>-0.085358</td>\n",
       "      <td>1</td>\n",
       "      <td>5b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-06-14 17:02:21.342869841-02:00</td>\n",
       "      <td>63.387314</td>\n",
       "      <td>-0.421361</td>\n",
       "      <td>-0.344314</td>\n",
       "      <td>0.504001</td>\n",
       "      <td>0.049513</td>\n",
       "      <td>-0.251242</td>\n",
       "      <td>0.149490</td>\n",
       "      <td>1</td>\n",
       "      <td>5b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-06-14 17:02:21.545000-02:00</td>\n",
       "      <td>63.588592</td>\n",
       "      <td>1.043850</td>\n",
       "      <td>0.042929</td>\n",
       "      <td>-0.344855</td>\n",
       "      <td>0.249277</td>\n",
       "      <td>-1.278707</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>1</td>\n",
       "      <td>5b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33791 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      entry_num                             datetime   Time (s)  X (m/s^2)  \\\n",
       "0            15     2023-06-07 16:36:31.299000-02:00   0.202257   0.616807   \n",
       "1            15  2023-06-07 16:36:31.399774729-02:00   0.403552   0.427085   \n",
       "2            15  2023-06-07 16:36:31.500549459-02:00   0.604846   0.570240   \n",
       "3            15  2023-06-07 16:36:31.601324189-02:00   0.806141   0.295633   \n",
       "4            15  2023-06-07 16:36:31.702098919-02:00   1.007435   0.206272   \n",
       "...         ...                                  ...        ...        ...   \n",
       "5186         40  2023-06-14 17:02:20.736479365-02:00  62.783481   0.138590   \n",
       "5187         40  2023-06-14 17:02:20.938609523-02:00  62.984758  -0.756452   \n",
       "5188         40  2023-06-14 17:02:21.140739682-02:00  63.186036  -0.321295   \n",
       "5189         40  2023-06-14 17:02:21.342869841-02:00  63.387314  -0.421361   \n",
       "5190         40     2023-06-14 17:02:21.545000-02:00  63.588592   1.043850   \n",
       "\n",
       "      Y (m/s^2)  Z (m/s^2)  X (rad/s)  Y (rad/s)  Z (rad/s)  name_climber  \\\n",
       "0      0.545096  -0.119170  -0.738735  -0.693665  -0.053444             0   \n",
       "1     -0.449999   0.075464  -0.314339  -0.378076   0.015612             0   \n",
       "2      0.152899  -0.213106  -0.273812  -0.442562  -0.130227             0   \n",
       "3     -0.453686   0.244799  -0.260747  -0.114135  -0.019194             0   \n",
       "4     -0.927811  -0.512616  -0.160211  -0.226173  -0.211577             0   \n",
       "...         ...        ...        ...        ...        ...           ...   \n",
       "5186  -1.738872  -0.396787  -0.371624  -0.194527  -0.130351             1   \n",
       "5187  -0.380505   0.312611   0.064059   0.111573  -0.053014             1   \n",
       "5188  -0.189666  -0.070279  -0.062600  -0.059749  -0.085358             1   \n",
       "5189  -0.344314   0.504001   0.049513  -0.251242   0.149490             1   \n",
       "5190   0.042929  -0.344855   0.249277  -1.278707   0.013051             1   \n",
       "\n",
       "     grading  num_attempt  fall_top  outdoor  Unnamed: 0  \n",
       "0         4c            1         0      0.0         NaN  \n",
       "1         4c            1         0      0.0         NaN  \n",
       "2         4c            1         0      0.0         NaN  \n",
       "3         4c            1         0      0.0         NaN  \n",
       "4         4c            1         0      0.0         NaN  \n",
       "...      ...          ...       ...      ...         ...  \n",
       "5186      5b            1         0      NaN         NaN  \n",
       "5187      5b            1         0      NaN         NaN  \n",
       "5188      5b            1         0      NaN         NaN  \n",
       "5189      5b            1         0      NaN         NaN  \n",
       "5190      5b            1         0      NaN         NaN  \n",
       "\n",
       "[33791 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cd.combine_start_dfs(data_sien, data_sien2, data_tim, data_tim2)\n",
    "# change name climber to numerical value\n",
    "data['name_climber'] = data['name_climber'].map({'sien':0,'tim':1})\n",
    "# drop weird column (SIEN's FAULT)\n",
    "data = data.drop(columns = ['Time (s).1'])\n",
    "# drop climber rating var \n",
    "data = cd.drop_climber_rating(data)\n",
    "# add datetime var \n",
    "data = cd.add_datetime(data)\n",
    "# add heart-rate var\n",
    "\n",
    "\n",
    "\n",
    "# cut falls \n",
    "# data = cd.cut_fall(data)\n",
    "\n",
    "# data = fe.features(data)\n",
    "# data.grading = data.grading.apply(fe.french_grade_to_num)\n",
    "\n",
    "# data = create_snippets(data, 20, 5)\n",
    "# data = change_climb_ID(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- time jumps are inconsistent (sometimes 5, sometimes 10)\n",
    "- extract & map heart rate values within a datetime range to their group -> dict? \n",
    "    - iterate through df groups\n",
    "        - iterate through json file \n",
    "        \n",
    "- fit values to entries in each group (will have some nans)\n",
    "- interpolate missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('heartrate/heart_rate-2023-06-07 copy.json', 'r') as f1: \n",
    "    hr_json1 = json.load(f1)\n",
    "with open('heartrate/heart_rate-2023-06-14 copy.json', 'r') as f2: \n",
    "    hr_json2 = json.load(f2)\n",
    "\n",
    "\n",
    "# convert entries to datetime obj & put values in list \n",
    "hr_data1 = [] \n",
    "for heart_entry in hr_json1:\n",
    "    dt = heart_entry['dateTime']\n",
    "    hr = heart_entry['value']['bpm']\n",
    "    hr_data1.append((dt, hr))\n",
    "\n",
    "hr_data2 = []\n",
    "for heart_entry in hr_json2:\n",
    "    dt = heart_entry['dateTime']\n",
    "    hr = heart_entry['value']['bpm']\n",
    "    hr_data2.append((dt, hr))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('heartrate/heart_rate-2023-06-07 copy.json', 'r') as f1: \n",
    "    hr_json1 = json.load(f1)\n",
    "with open('heartrate/heart_rate-2023-06-14 copy.json', 'r') as f2: \n",
    "    hr_json2 = json.load(f2)\n",
    "\n",
    "\n",
    "# convert entries to datetime obj & put values in list \n",
    "hr_data1 = [] \n",
    "for heart_entry in hr_json1:\n",
    "    dt = heart_entry['dateTime']\n",
    "    hr = heart_entry['value']['bpm']\n",
    "    hr_data1.append((dt, hr))\n",
    "\n",
    "hr_data2 = []\n",
    "for heart_entry in hr_json2:\n",
    "    dt = heart_entry['dateTime']\n",
    "    hr = heart_entry['value']['bpm']\n",
    "    hr_data2.append((dt, hr))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=datetime64[ns] and Timestamp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:582\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_compatible_with(other)\n\u001b[1;32m    583\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    584\u001b[0m     \u001b[39m# e.g. tzawareness mismatch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:461\u001b[0m, in \u001b[0;36mDatetimeArray._check_compatible_with\u001b[0;34m(self, other, setitem)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assert_tzawareness_compat(other)\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m setitem:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# Stricter check for setitem vs comparison methods\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:694\u001b[0m, in \u001b[0;36mDatetimeArray._assert_tzawareness_compat\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m other_tz \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    695\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot compare tz-naive and tz-aware datetime-like objects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m \u001b[39melif\u001b[39;00m other_tz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot compare tz-naive and tz-aware datetime-like objects.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidComparison\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:1054\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1054\u001b[0m     other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_comparison_value(other)\n\u001b[1;32m   1055\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidComparison:\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:585\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    584\u001b[0m         \u001b[39m# e.g. tzawareness mismatch\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidComparison(other) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(other):\n",
      "\u001b[0;31mInvalidComparison\u001b[0m: 2023-06-07 13:28:35.606000-02:00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m start_time \u001b[39m=\u001b[39m group_data\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m end_time \u001b[39m=\u001b[39m group_data\u001b[39m.\u001b[39miloc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m filtered_dt \u001b[39m=\u001b[39m fit_datetimes[(fit_datetimes \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m start_time) \u001b[39m&\u001b[39m (fit_datetimes \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m end_time)]\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(start_time, end_time)\n\u001b[1;32m     29\u001b[0m group_data[[\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTime (s)\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mtemp.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arraylike.py:62\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__ge__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ge__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49mge)\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6975\u001b[0m, in \u001b[0;36mIndex._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6972\u001b[0m         result \u001b[39m=\u001b[39m op(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values, other)\n\u001b[1;32m   6974\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values, ExtensionArray):\n\u001b[0;32m-> 6975\u001b[0m     result \u001b[39m=\u001b[39m op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, other)\n\u001b[1;32m   6977\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCMultiIndex):\n\u001b[1;32m   6978\u001b[0m     \u001b[39m# don't pass MultiIndex\u001b[39;00m\n\u001b[1;32m   6979\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arraylike.py:62\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__ge__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ge__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49mge)\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:1056\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_comparison_value(other)\n\u001b[1;32m   1055\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidComparison:\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(\u001b[39mself\u001b[39;49m, other, op)\n\u001b[1;32m   1058\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(other, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[1;32m   1060\u001b[0m     \u001b[39m# We have to use comp_method_OBJECT_ARRAY instead of numpy\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m     \u001b[39m#  comparison otherwise it would fail to raise when\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m     \u001b[39m#  comparing tz-aware and tz-naive\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VU/uni_venv/lib/python3.11/site-packages/pandas/core/ops/invalid.py:36\u001b[0m, in \u001b[0;36minvalid_comparison\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     typ \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(right)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid comparison between dtype=\u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mtyp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid comparison between dtype=datetime64[ns] and Timestamp"
     ]
    }
   ],
   "source": [
    "fit_datetimes = [x[0] for x in hr_data1]\n",
    "fit_hr = [x[1] for x in hr_data1]\n",
    "\n",
    "# convert fitbit times to datetime obj\n",
    "fit_datetimes = pd.to_datetime(fit_datetimes)\n",
    "\n",
    "# NOTE: OK! but need to do this for only the datetimes within a certain interval, cuz otherwise it's gonna match each row in the df\n",
    "# NOTE: maybe try not to merge dfs, but init a df and then do the masks\n",
    "# NOTE: adapt this to extract heartrate shit then  \n",
    "# NOTE: it seems like 1 climb had no heart rate measurements (oh wait it's cuz I'm ony using part of the data)\n",
    "# NOTE: make sure to do interpolation per grouping \n",
    "# NOTE: datetime in data is every 100ms now instead of 200ms \n",
    "\n",
    "# inits null heartrate variable \n",
    "# if not 'heart-rate' in data:\n",
    "#     data.insert(2, 'heart-rate', float(\"nan\"))\n",
    "\n",
    "# group by climbID & get datetime intervals \n",
    "for group_name, group_data in data.groupby(\"entry_num\"):\n",
    "    print(group_name)\n",
    "    \n",
    "    # gets all fitbit datetimes within the given interval \n",
    "    start_time = group_data.iloc[0]['datetime']\n",
    "    end_time = group_data.iloc[-1]['datetime']\n",
    "    filtered_dt = fit_datetimes[(fit_datetimes >= start_time) & (fit_datetimes <= end_time)]\n",
    "\n",
    "    print(start_time, end_time)\n",
    "    group_data[['datetime', 'Time (s)']].to_csv('temp.csv')\n",
    "    \n",
    "\n",
    "    # gets heart-rate values for given datetime interval \n",
    "    filtered_idxs = np.where(np.isin(fit_datetimes, filtered_dt))[0]\n",
    "    filtered_hr = fit_hr[filtered_idxs[0]:filtered_idxs[-1] + 1]\n",
    "\n",
    "    # TODO: calculation here seems to mess up -> when moving to new minute\n",
    "\n",
    "\n",
    "    # map & retrieve closests df datetimes to fitbit datetimes per group \n",
    "    # # print(filtered_hr)\n",
    "    # # make df out of fitbit dt and hr \n",
    "    # filtered_df = pd.DataFrame(list(zip(filtered_dt, filtered_hr)), columns=['datetime', 'heart-rate'])\n",
    "    # print(filtered_df)\n",
    "    # merged_df = pd.merge_asof(group_data, filtered_df, left_on='datetime', right_on='datetime', direction='nearest', suffixes=('', '_drop'))\n",
    "    # merged_df = merged_df.drop(columns=merged_df.filter(regex='_drop$').columns)\n",
    "    # print(merged_df)\n",
    "    # merged_df.to_csv('temp.csv')\n",
    "\n",
    "\n",
    "    # -- old -- \n",
    "    data = data.sort_values('datetime')\n",
    "\n",
    "    closest_datetimes = filtered_dt.map(lambda x: data['datetime'].iloc[(data['datetime'] - x).abs().idxmin()])\n",
    "\n",
    "\n",
    "    comp_dt = [(f'fit: {i}', f'cloests {j}') for i, j in zip(filtered_dt, closest_datetimes)]\n",
    "    print(comp_dt)\n",
    "\n",
    "\n",
    "    # # mask based on the matching datetime values\n",
    "    # mask = data['datetime'].isin(closest_datetimes)\n",
    "    # # adding heart-rate values to df \n",
    "    # filtered_dt = data[mask]\n",
    "    # data.loc[mask, 'C'] = filtered_hr\n",
    "\n",
    "    break \n",
    "\n",
    "# TODO: I don't think this is working quite right ... \n",
    "temp = data[data['entry_num'] == 1]\n",
    "temp\n",
    "\n",
    "    #     # fills in datetime sequences\n",
    "    # for entry_id in df_copy[\"entry_num\"].unique():\n",
    "    #     mask = df_copy[\"entry_num\"] == entry_id\n",
    "    #     df_copy.loc[mask, \"datetime\"] = datetime_sequence_dict[entry_id]\n",
    "\n",
    "    \n",
    "\n",
    "    # Filter the dataframe based on the datetime interval\n",
    "    # filtered_dt = data[(df['timestamp'] >= start_datetime) & (df['timestamp'] <= end_datetime)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find closest respective dates\n",
    "# closest_dates = fit_datetimes.map(lambda x: data['datetime'].iloc[(data['datetime'] - x).abs().idxmin()])\n",
    "# print(closest_dates)\n",
    "\n",
    "\n",
    "# # Create a DataFrame with the closest respective dates\n",
    "# df_new = pd.DataFrame({'closest_datetime': closest_dates, 'new_datetime': fit_datetimes})\n",
    "\n",
    "# # Merge the original DataFrame with the closest respective dates\n",
    "# merged_df = pd.merge(data, df_new, left_on='datetime', right_on='closest_datetime', how='outer')\n",
    "\n",
    "# merged_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_datetimes = [x[0] for x in hr_data1]\n",
    "hr = [x[1] for x in hr_data1]\n",
    "\n",
    "# iterate through climb IDs\n",
    "for group_name, group_data in data.groupby(\"entry_num\"):\n",
    "    print(group_name)\n",
    "    print(group_data.iloc[0]['datetime'])\n",
    "    print(group_data.iloc[-1]['datetime'])\n",
    "\n",
    "    new_datetimes = pd.to_datetime(new_datetimes)\n",
    "\n",
    "    # closest_dates = new_datetimes.map(lambda x: group_data['datetime'].iloc[(group_data['datetime'] - x).abs().idxmin()])\n",
    "\n",
    "\n",
    "    # iterate through heart data & match entries by closest datetime \n",
    "    # for heart_entry in heart_data1:\n",
    "    #     print(heart_entry)\n",
    "    \n",
    "\n",
    "    # print(type(heart_data1[0]['dateTime']))\n",
    "    # for entry in heart_data1:\n",
    "        # if entry['dateTime'] in group_data['datetime']:\n",
    "        #     print('asdfasdf')\n",
    "    break   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Original DataFrame with datetime entries\n",
    "df_original = pd.DataFrame({\n",
    "    'datetime_original': pd.to_datetime(['2023-06-19 12:00:00', '2023-06-19 13:30:00', '2023-06-19 14:45:00']),\n",
    "    'value_original': [1, 2, 3]\n",
    "})\n",
    "\n",
    "# New datetime entries\n",
    "new_datetimes = pd.to_datetime(['2023-06-19 12:15:00', '2023-06-19 13:40:00'])\n",
    "print(new_datetimes)\n",
    "\n",
    "# Find closest respective dates\n",
    "closest_dates = new_datetimes.map(lambda x: df_original['datetime_original'].iloc[(df_original['datetime_original'] - x).abs().idxmin()])\n",
    "\n",
    "# Create a DataFrame with the closest respective dates\n",
    "df_new = pd.DataFrame({'closest_datetime': closest_dates, 'new_datetime': new_datetimes})\n",
    "\n",
    "# Merge the original DataFrame with the closest respective dates\n",
    "merged_df = pd.merge(df_original, df_new, left_on='datetime_original', right_on='closest_datetime', how='outer')\n",
    "\n",
    "merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tim = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_tim_copy.csv')\n",
    "data_sien = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_sien_copy.csv')\n",
    "data_tim2 = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_tim_2_copy.csv')\n",
    "data_sien2 = pd.read_csv('/Users/ryonamba/Documents/VU/VU-master_year_1/ML_quantified_self/ML4QS/datasets/raw_data_sien_copy.csv')\n",
    "\n",
    "\n",
    "data_tim = data_tim.drop(columns = ['Unnamed: 0'])\n",
    "data_sien = data_sien.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = cd.combine_start_dfs(data_sien, data_sien2, data_tim, data_tim2)\n",
    "df['name_climber'] = df['name_climber'].map({'sien':0,'tim':1})\n",
    "df\n",
    "df = df.drop(columns = ['Time (s).1'])\n",
    "df = cd.drop_climber_rating(df)\n",
    "df = cd.add_datetime(df)\n",
    "df = cd.cut_fall(df)\n",
    "\n",
    "\n",
    "df.grading = df.grading.apply(fe.french_grade_to_num)\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from DMT\n",
    "# datatypes - NOTE: subject to change (only accelerometer & gyrometer rn)\n",
    "types = ['id','datetime', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'categorical', 'ordinal', ] \n",
    "\n",
    "# nb of values \n",
    "n_values = [data[var].count() for var in data]\n",
    "\n",
    "# nb of unique values\n",
    "n_unique = [data[var].nunique() for var in data]\n",
    "\n",
    "\n",
    "# TODO: do once we have the final variables (eg. for grade)\n",
    "## mean + std \n",
    "# mean_std = []\n",
    "\n",
    "# for var in data: \n",
    "#     if var not in ['datetime', 'entry_num']: # TODO \n",
    "#         mean = round(data[var].mean(), 2)\n",
    "#         std = round(data[var].std(), 2) \n",
    "#         res = f'{mean} ({std})'\n",
    "#         mean_std.append(res)\n",
    "# mean_std.insert(0,'Na')\n",
    "# mean_std.insert(0,'Na')\n",
    "\n",
    "# # range of values\n",
    "# val_range = []\n",
    "# for var in data:\n",
    "#     if var != 'id':\n",
    "#         r = (data[var].min(), data[var].max())\n",
    "#         if var != 'time':\n",
    "#             r = (round(r[0]), round(r[1]))\n",
    "#         else:\n",
    "#             r = (data['time'].dt.date.min(), data['time'].dt.date.max())\n",
    "#         val_range.append(r)\n",
    "#         r = data['time'].dt.date\n",
    "# val_range.insert(0, 'Na')\n",
    "\n",
    "# # date range \n",
    "# time_range = []\n",
    "# for var in data: \n",
    "#     # if var != 'id':\n",
    "#     grouped = data.groupby(var)['time'].min()\n",
    "#     r = (grouped.dt.date.min(), grouped.dt.date.max())\n",
    "#     time_range.append(r)\n",
    "\n",
    "\n",
    "# # missing values before time aggregation TODO: do for heartrate only ig\n",
    "# missing_before_agg = data.isna().sum().tolist()\n",
    "\n",
    "# # missing values after time aggregation -> fill this in manually... \n",
    "# # automatic 14 for each except call & sms \n",
    "# # + 13 for activity \n",
    "# # + 1 for 'circumplex.arousal','circumplex.valence', 'mood' & -2 cuz there were like 2 days of measurements in the first 14 days? \n",
    "# # temp_data = fn.group_data(data, count=False)\n",
    "# # missing_after_agg = temp_data.isna().sum().tolist()\n",
    "# # missing_after_agg.insert(0, 0)\n",
    "\n",
    "\n",
    "# missing_after_agg = [0 for i in range(len(data.columns))]\n",
    "# for i, var in enumerate(data.columns):\n",
    "#     if var == 'sms' or var == 'call' or var == 'id' or var == 'time': \n",
    "#         continue\n",
    "#     if var == 'circumplex.arousal' or var == 'circumplex.valence' or var == 'mood':\n",
    "#         missing_after_agg[i] += 1\n",
    "#     missing_after_agg[i] += 14\n",
    "#     if var == 'activity':\n",
    "#         missing_after_agg[i] += 13\n",
    "\n",
    "\n",
    "\n",
    "data_table = pd.DataFrame({'Variables': data.columns, \n",
    "                    # 'Data types': types, \n",
    "                    'number values': n_values, \n",
    "                    'number unique values': n_unique, \n",
    "                    # 'Mean (std)': mean_std,\n",
    "                    # 'Value range': val_range,\n",
    "                    # 'Date range': time_range, \n",
    "                    # 'Missing values (before time aggregation)': missing_before_agg,\n",
    "                    # 'Missing values (after time aggregation)': missing_after_agg\n",
    "                    })\n",
    "data_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
